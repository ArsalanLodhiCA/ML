{"cells":[{"cell_type":"markdown","source":["## Part 1: Data Ingestion\nDirectly importing from S3"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2bdd362f-e4bc-4fbe-8be5-4e6d45071eb3","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["import os\nimport boto3\n\n# Set the AWS access key and secret key\nos.environ['AWS_ACCESS_KEY_ID'] = 'AKIAZ5HTAWGZVGQTHD64'\nos.environ['AWS_SECRET_ACCESS_KEY'] = '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'\n\n# Create an S3 client\ns3 = boto3.client('s3')\n\n# Set the name of the S3 bucket\nbucket_name = 'amazon-reviews'\n\n# Try to list the contents of the bucket\ntry:\n    # Use the S3 client to list the objects in the bucket\n    response = s3.list_objects(Bucket=bucket_name)\n    print(f'Access to S3 bucket {bucket_name} is successful')\n    print(response)\nexcept Exception as e:\n    print(f'Error: {e}')\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"396b0a89-718a-44b3-b4b1-1993a9d87fc6","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Access to S3 bucket amazon-reviews is successful\n{'ResponseMetadata': {'RequestId': 'HJ3B0KA3Y4G6KRVT', 'HostId': 'ujKGbOrKr5VWxTfGnir5Do9PTz+AuUfLOGWK2gCSPGcIBKcSgQG0k8U8GVCGaCDH7rp+gHiozAY=', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amz-id-2': 'ujKGbOrKr5VWxTfGnir5Do9PTz+AuUfLOGWK2gCSPGcIBKcSgQG0k8U8GVCGaCDH7rp+gHiozAY=', 'x-amz-request-id': 'HJ3B0KA3Y4G6KRVT', 'date': 'Fri, 20 Jan 2023 03:26:32 GMT', 'x-amz-bucket-region': 'us-west-2', 'content-type': 'application/xml', 'transfer-encoding': 'chunked', 'server': 'AmazonS3'}, 'RetryAttempts': 1}, 'IsTruncated': False, 'Marker': '', 'Contents': [{'Key': 'amazon_reviews_us_Books_v1_00.tsv.gz', 'LastModified': datetime.datetime(2023, 1, 17, 16, 38, 4, tzinfo=tzlocal()), 'ETag': '\"853da645a25daf3d3c86559dde032f82-160\"', 'Size': 2740337188, 'StorageClass': 'STANDARD', 'Owner': {'DisplayName': 'info', 'ID': 'f164a2c4b766eb258859f6a107c41f628bbcad023221405d299ecb211ca16cb0'}}], 'Name': 'amazon-reviews', 'Prefix': '', 'MaxKeys': 1000, 'EncodingType': 'url'}\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Access to S3 bucket amazon-reviews is successful\n{'ResponseMetadata': {'RequestId': 'HJ3B0KA3Y4G6KRVT', 'HostId': 'ujKGbOrKr5VWxTfGnir5Do9PTz+AuUfLOGWK2gCSPGcIBKcSgQG0k8U8GVCGaCDH7rp+gHiozAY=', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amz-id-2': 'ujKGbOrKr5VWxTfGnir5Do9PTz+AuUfLOGWK2gCSPGcIBKcSgQG0k8U8GVCGaCDH7rp+gHiozAY=', 'x-amz-request-id': 'HJ3B0KA3Y4G6KRVT', 'date': 'Fri, 20 Jan 2023 03:26:32 GMT', 'x-amz-bucket-region': 'us-west-2', 'content-type': 'application/xml', 'transfer-encoding': 'chunked', 'server': 'AmazonS3'}, 'RetryAttempts': 1}, 'IsTruncated': False, 'Marker': '', 'Contents': [{'Key': 'amazon_reviews_us_Books_v1_00.tsv.gz', 'LastModified': datetime.datetime(2023, 1, 17, 16, 38, 4, tzinfo=tzlocal()), 'ETag': '\"853da645a25daf3d3c86559dde032f82-160\"', 'Size': 2740337188, 'StorageClass': 'STANDARD', 'Owner': {'DisplayName': 'info', 'ID': 'f164a2c4b766eb258859f6a107c41f628bbcad023221405d299ecb211ca16cb0'}}], 'Name': 'amazon-reviews', 'Prefix': '', 'MaxKeys': 1000, 'EncodingType': 'url'}\n"]}}],"execution_count":0},{"cell_type":"code","source":["import pyspark\nfrom delta import *\nfrom pyspark.sql import SparkSession\n\n# Define the input and output formats and paths and the table name.\nwrite_format = 'delta'\n#load_path = 'file:/databricks/driver/bank.csv'\nsave_path = 'file:/databricks/driver/tmp/delta/amazon-reviews-2g'\ntable_name = 'default.amazonreviews2g'\n\n\n# Define the S3 path to the TSV.gz file\ns3_path = \"s3a://amazon-reviews/amazon_reviews_us_Books_v1_00.tsv.gz\"\n\n# Create a SparkSession\nbuilder = pyspark.sql.SparkSession.builder.appName(\"amazon-reviews\") \\\n  .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n  .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n\n#spark = SparkSession.builder.appName(\"amazon-reviews\").getOrCreate()\n\nspark = configure_spark_with_delta_pip(builder).getOrCreate()\n\n\n# Create a DataFrame from the TSV.gz file\ndf = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"delimiter\", \"\\t\").load(s3_path)\n\n# Show the DataFrame\n#df.show()\n\n#df.printSchema()\n\ndf.createOrReplaceTempView(table_name)\n# df.write.saveAsTable(table_name, path=save_path)\n\ndf.write.mode(\"overwrite\").saveAsTable(table_name, path=save_path)\n\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"058e7e50-aef4-4444-a45e-7358ce4bf465","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["ls -lh /databricks/driver/tmp/delta/amazon-reviews-2g"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a51d26ff-99a5-4846-829b-70d5e701de0c","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"total 3.9G\r\ndrwxr-xr-x 2 root root 4.0K Jan 20 03:31 \u001B[0m\u001B[01;34m_delta_log\u001B[0m/\r\n-rw-r--r-- 1 root root 3.9G Jan 20 03:31 part-00000-ce14c406-1ab6-4c82-9152-fc27f27d8755-c000.snappy.parquet\r\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["total 3.9G\r\ndrwxr-xr-x 2 root root 4.0K Jan 20 03:31 \u001B[0m\u001B[01;34m_delta_log\u001B[0m/\r\n-rw-r--r-- 1 root root 3.9G Jan 20 03:31 part-00000-ce14c406-1ab6-4c82-9152-fc27f27d8755-c000.snappy.parquet\r\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Part 2: Exploring The Data"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"7f796a76-f310-4409-9001-a7178504acb0","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["read_format = 'delta'\nload_path = 'file:/databricks/driver/tmp/delta/amazon-reviews-2g'\n\n# read from the table instead of s3\ndf = spark.read.format(read_format).option(\"inferSchema\", \"true\").load(load_path)\ndf.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a93bf93f-9472-4cfe-88db-cb0c8604baf9","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n |-- marketplace: string (nullable = true)\n |-- customer_id: string (nullable = true)\n |-- review_id: string (nullable = true)\n |-- product_id: string (nullable = true)\n |-- product_parent: string (nullable = true)\n |-- product_title: string (nullable = true)\n |-- product_category: string (nullable = true)\n |-- star_rating: string (nullable = true)\n |-- helpful_votes: string (nullable = true)\n |-- total_votes: string (nullable = true)\n |-- vine: string (nullable = true)\n |-- verified_purchase: string (nullable = true)\n |-- review_headline: string (nullable = true)\n |-- review_body: string (nullable = true)\n |-- review_date: string (nullable = true)\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- marketplace: string (nullable = true)\n |-- customer_id: string (nullable = true)\n |-- review_id: string (nullable = true)\n |-- product_id: string (nullable = true)\n |-- product_parent: string (nullable = true)\n |-- product_title: string (nullable = true)\n |-- product_category: string (nullable = true)\n |-- star_rating: string (nullable = true)\n |-- helpful_votes: string (nullable = true)\n |-- total_votes: string (nullable = true)\n |-- vine: string (nullable = true)\n |-- verified_purchase: string (nullable = true)\n |-- review_headline: string (nullable = true)\n |-- review_body: string (nullable = true)\n |-- review_date: string (nullable = true)\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["# Count the number of rows where the \"date\" column is null\nnull_rows = df.filter(df[\"review_date\"].isNull()).count()\n\n# Print the result\nprint(\"Number of rows with null date:\", null_rows)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5ee9062a-deb3-4e31-b9eb-29aa596a62b2","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Number of rows with null date: 2288\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Number of rows with null date: 2288\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Part 2.1: Feature Engineering"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a9f5b5ce-6e70-40e5-a44e-8cd316366310","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["After a brief inspection of the data, we found there are a series of data pre-processing we have to conduct.\n\n* Remove the “Title” feature.\n* Remove the rows where “Review Text” were missing.\n* Clean “Review Text” column.\n* Determine the best method to calculate sentiment polarity"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b609adc1-85ea-4659-9053-5cb2a31b74e2","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.sql.functions import regexp_replace, trim\n\ndf = df.drop('marketplace')\ndf = df.drop(*['customer_id', 'review_id', 'product_id'])\ndf = df.drop(*['product_parent', 'product_category', 'helpful_votes', 'total_votes', 'vine', 'verified_purchase'])\n\ndf = df.dropna(subset=[\"review_body\"])\n\ndef preprocess_df(df):\n    df = df.withColumn(\"review_body\", regexp_replace(\"review_body\", \"(<br/>)\", \"\"))\n    df = df.withColumn(\"review_body\", regexp_replace(\"review_body\", \"(<a).*(>).*(</a>)\", \"\"))\n    df = df.withColumn(\"review_body\", regexp_replace(\"review_body\", \"(&amp)\", \"\"))\n    df = df.withColumn(\"review_body\", regexp_replace(\"review_body\", \"(&gt)\", \"\"))\n    df = df.withColumn(\"review_body\", regexp_replace(\"review_body\", \"(&lt)\", \"\"))\n    df = df.withColumn(\"review_body\", regexp_replace(\"review_body\", \"(\\\\xa0)\", \" \"))\n    return df\n\ndf = preprocess_df(df)\n\ndf.show()\n\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"6984bec2-9303-44be-9baa-f2f731fbe4b3","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+--------------------+-----------+--------------------+--------------------+-----------+\n|       product_title|star_rating|     review_headline|         review_body|review_date|\n+--------------------+-----------+--------------------+--------------------+-----------+\n|There Was an Old ...|          5|          Five Stars|I love it and so ...| 2015-08-31|\n|      I Saw a Friend|          5|Please buy \"I Saw...|My wife and I ord...| 2015-08-31|\n|Black Lagoon, Vol. 6|          5|       Shipped fast.|Great book just l...| 2015-08-31|\n|           If I Stay|          5|          Five Stars|        So beautiful| 2015-08-31|\n|Stars 'N Strips F...|          5|          Five Stars|Enjoyed the autho...| 2015-08-31|\n|            The Liar|          2|PREDICTABLE ALMOS...|Two or three page...| 2015-08-31|\n|Devil in the Deta...|          5|The Monastery Mur...|&#34;Secrets in t...| 2015-08-31|\n|Knowing When to S...|          5|          Five Stars|          I love it!| 2015-08-31|\n|The American Pageant|          5|          Five Stars|It was a great pu...| 2015-08-31|\n|Punjabi C.L. Bibl...|          5|          Five Stars|Quality product f...| 2015-08-31|\n|The Aeneid (Every...|          5|          Five Stars|          Very happy| 2015-08-31|\n|The YMCA of Middl...|          5|Great history boo...|Love reading all ...| 2015-08-31|\n|Memoirs By Harry ...|          5|          Five Stars|such a great purc...| 2015-08-31|\n|Arms of our fight...|          4|          Four Stars|book was fine Tha...| 2015-08-31|\n|Presbyterian Cree...|          3|The Presbyterian ...|The Presbyterian ...| 2015-08-31|\n|Hello Ocean/Hola mar|          5|          Five Stars|beautiful work an...| 2015-08-31|\n|The Vampire Lesta...|          5|       Shipped fast.|This is my favori...| 2015-08-31|\n|The Book of Life ...|          5|One of my favorites!|Love Deborah Harn...| 2015-08-31|\n|Disney Descendant...|          5|... has all three...|Now my daughter h...| 2015-08-31|\n|CompTIA A+ Certif...|          5|He said it has be...|Bought this book ...| 2015-08-31|\n+--------------------+-----------+--------------------+--------------------+-----------+\nonly showing top 20 rows\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+--------------------+-----------+--------------------+--------------------+-----------+\n|       product_title|star_rating|     review_headline|         review_body|review_date|\n+--------------------+-----------+--------------------+--------------------+-----------+\n|There Was an Old ...|          5|          Five Stars|I love it and so ...| 2015-08-31|\n|      I Saw a Friend|          5|Please buy \"I Saw...|My wife and I ord...| 2015-08-31|\n|Black Lagoon, Vol. 6|          5|       Shipped fast.|Great book just l...| 2015-08-31|\n|           If I Stay|          5|          Five Stars|        So beautiful| 2015-08-31|\n|Stars 'N Strips F...|          5|          Five Stars|Enjoyed the autho...| 2015-08-31|\n|            The Liar|          2|PREDICTABLE ALMOS...|Two or three page...| 2015-08-31|\n|Devil in the Deta...|          5|The Monastery Mur...|&#34;Secrets in t...| 2015-08-31|\n|Knowing When to S...|          5|          Five Stars|          I love it!| 2015-08-31|\n|The American Pageant|          5|          Five Stars|It was a great pu...| 2015-08-31|\n|Punjabi C.L. Bibl...|          5|          Five Stars|Quality product f...| 2015-08-31|\n|The Aeneid (Every...|          5|          Five Stars|          Very happy| 2015-08-31|\n|The YMCA of Middl...|          5|Great history boo...|Love reading all ...| 2015-08-31|\n|Memoirs By Harry ...|          5|          Five Stars|such a great purc...| 2015-08-31|\n|Arms of our fight...|          4|          Four Stars|book was fine Tha...| 2015-08-31|\n|Presbyterian Cree...|          3|The Presbyterian ...|The Presbyterian ...| 2015-08-31|\n|Hello Ocean/Hola mar|          5|          Five Stars|beautiful work an...| 2015-08-31|\n|The Vampire Lesta...|          5|       Shipped fast.|This is my favori...| 2015-08-31|\n|The Book of Life ...|          5|One of my favorites!|Love Deborah Harn...| 2015-08-31|\n|Disney Descendant...|          5|... has all three...|Now my daughter h...| 2015-08-31|\n|CompTIA A+ Certif...|          5|He said it has be...|Bought this book ...| 2015-08-31|\n+--------------------+-----------+--------------------+--------------------+-----------+\nonly showing top 20 rows\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["# Using pipeline class to make predictions from models available in the Hub in an easy way \nfrom transformers import pipeline\nfrom pyspark.sql.functions import col\nfrom transformers import pipeline\nfrom pyspark.sql.types import StructType, StructField, StringType\nfrom pyspark.sql.functions import array\n\n# Initialize the sentiment analysis pipeline\nsentiment_pipeline = pipeline('sentiment-analysis', model='finiteautomata/bertweet-base-sentiment-analysis')\n\n# Apply the pipeline to each review\n# sentiments = df.rdd.map(lambda row: sentiment_pipeline(row['review_body'])[0]['label']).toDF(['sentiment'])\nsentiments = df.rdd.map()\n\n# Add the sentiment column back to the original dataframe\ndf = df.withColumn('sentiment', sentiments['sentiment'])\n\n\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"7686e7d8-24b9-45c3-9dbb-ab71a1d17b6c","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"emoji is not installed, thus not converting emoticons or emojis into text. Install emoji: pip3 install emoji==0.6.0\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["emoji is not installed, thus not converting emoticons or emojis into text. Install emoji: pip3 install emoji==0.6.0\n"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)\nFile \u001B[0;32m<command-2700848159738089>:12\u001B[0m\n\u001B[1;32m      9\u001B[0m sentiment_pipeline \u001B[38;5;241m=\u001B[39m pipeline(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msentiment-analysis\u001B[39m\u001B[38;5;124m'\u001B[39m, model\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfiniteautomata/bertweet-base-sentiment-analysis\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     11\u001B[0m \u001B[38;5;66;03m# Apply the pipeline to each review\u001B[39;00m\n\u001B[0;32m---> 12\u001B[0m sentiments \u001B[38;5;241m=\u001B[39m df\u001B[38;5;241m.\u001B[39mrdd\u001B[38;5;241m.\u001B[39mmap(\u001B[38;5;28;01mlambda\u001B[39;00m row: sentiment_pipeline(row[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mreview_body\u001B[39m\u001B[38;5;124m'\u001B[39m])[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlabel\u001B[39m\u001B[38;5;124m'\u001B[39m])\u001B[38;5;241m.\u001B[39mtoDF([\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msentiment\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[1;32m     14\u001B[0m \u001B[38;5;66;03m# Add the sentiment column back to the original dataframe\u001B[39;00m\n\u001B[1;32m     15\u001B[0m df \u001B[38;5;241m=\u001B[39m df\u001B[38;5;241m.\u001B[39mwithColumn(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msentiment\u001B[39m\u001B[38;5;124m'\u001B[39m, sentiments[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msentiment\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:103\u001B[0m, in \u001B[0;36m_monkey_patch_RDD.<locals>.toDF\u001B[0;34m(self, schema, sampleRatio)\u001B[0m\n\u001B[1;32m     75\u001B[0m \u001B[38;5;129m@no_type_check\u001B[39m\n\u001B[1;32m     76\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtoDF\u001B[39m(\u001B[38;5;28mself\u001B[39m, schema\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, sampleRatio\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m     77\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     78\u001B[0m \u001B[38;5;124;03m    Converts current :class:`RDD` into a :class:`DataFrame`\u001B[39;00m\n\u001B[1;32m     79\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    101\u001B[0m \u001B[38;5;124;03m    [Row(name='Alice', age=1)]\u001B[39;00m\n\u001B[1;32m    102\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 103\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43msparkSession\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcreateDataFrame\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mschema\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msampleRatio\u001B[49m\u001B[43m)\u001B[49m\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:972\u001B[0m, in \u001B[0;36mSparkSession.createDataFrame\u001B[0;34m(self, data, schema, samplingRatio, verifySchema)\u001B[0m\n\u001B[1;32m    967\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_pandas \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data, pd\u001B[38;5;241m.\u001B[39mDataFrame):\n\u001B[1;32m    968\u001B[0m     \u001B[38;5;66;03m# Create a DataFrame from pandas DataFrame.\u001B[39;00m\n\u001B[1;32m    969\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m(SparkSession, \u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39mcreateDataFrame(  \u001B[38;5;66;03m# type: ignore[call-overload]\u001B[39;00m\n\u001B[1;32m    970\u001B[0m         data, schema, samplingRatio, verifySchema\n\u001B[1;32m    971\u001B[0m     )\n\u001B[0;32m--> 972\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_create_dataframe\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    973\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mschema\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msamplingRatio\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverifySchema\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# type: ignore[arg-type]\u001B[39;49;00m\n\u001B[1;32m    974\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1020\u001B[0m, in \u001B[0;36mSparkSession._create_dataframe\u001B[0;34m(self, data, schema, samplingRatio, verifySchema)\u001B[0m\n\u001B[1;32m   1018\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1019\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data, RDD):\n\u001B[0;32m-> 1020\u001B[0m         rdd, struct \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_createFromRDD\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmap\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprepare\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mschema\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msamplingRatio\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1021\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1022\u001B[0m         rdd, struct \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_createFromLocal(\u001B[38;5;28mmap\u001B[39m(prepare, data), schema)\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:607\u001B[0m, in \u001B[0;36mSparkSession._createFromRDD\u001B[0;34m(self, rdd, schema, samplingRatio)\u001B[0m\n\u001B[1;32m    603\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    604\u001B[0m \u001B[38;5;124;03mCreate an RDD for DataFrame from an existing RDD, returns the RDD and schema.\u001B[39;00m\n\u001B[1;32m    605\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    606\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m schema \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(schema, (\u001B[38;5;28mlist\u001B[39m, \u001B[38;5;28mtuple\u001B[39m)):\n\u001B[0;32m--> 607\u001B[0m     struct \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_inferSchema\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrdd\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msamplingRatio\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnames\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mschema\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    608\u001B[0m     converter \u001B[38;5;241m=\u001B[39m _create_converter(struct)\n\u001B[1;32m    609\u001B[0m     tupled_rdd \u001B[38;5;241m=\u001B[39m rdd\u001B[38;5;241m.\u001B[39mmap(converter)\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:560\u001B[0m, in \u001B[0;36mSparkSession._inferSchema\u001B[0;34m(self, rdd, samplingRatio, names)\u001B[0m\n\u001B[1;32m    558\u001B[0m prefer_timestamp_ntz \u001B[38;5;241m=\u001B[39m is_timestamp_ntz_preferred()\n\u001B[1;32m    559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m samplingRatio \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 560\u001B[0m     schema \u001B[38;5;241m=\u001B[39m \u001B[43m_infer_schema\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    561\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfirst\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    562\u001B[0m \u001B[43m        \u001B[49m\u001B[43mnames\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnames\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    563\u001B[0m \u001B[43m        \u001B[49m\u001B[43minfer_dict_as_struct\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minfer_dict_as_struct\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    564\u001B[0m \u001B[43m        \u001B[49m\u001B[43mprefer_timestamp_ntz\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mprefer_timestamp_ntz\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    565\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    566\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m _has_nulltype(schema):\n\u001B[1;32m    567\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m row \u001B[38;5;129;01min\u001B[39;00m rdd\u001B[38;5;241m.\u001B[39mtake(\u001B[38;5;241m100\u001B[39m)[\u001B[38;5;241m1\u001B[39m:]:\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/types.py:1374\u001B[0m, in \u001B[0;36m_infer_schema\u001B[0;34m(row, names, infer_dict_as_struct, prefer_timestamp_ntz)\u001B[0m\n\u001B[1;32m   1371\u001B[0m     items \u001B[38;5;241m=\u001B[39m \u001B[38;5;28msorted\u001B[39m(row\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__dict__\u001B[39m\u001B[38;5;241m.\u001B[39mitems())\n\u001B[1;32m   1373\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1374\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCan not infer schema for type: \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m \u001B[38;5;28mtype\u001B[39m(row))\n\u001B[1;32m   1376\u001B[0m fields \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m   1377\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m items:\n\n\u001B[0;31mTypeError\u001B[0m: Can not infer schema for type: <class 'str'>","errorSummary":"<span class='ansi-red-fg'>TypeError</span>: Can not infer schema for type: <class 'str'>","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)\nFile \u001B[0;32m<command-2700848159738089>:12\u001B[0m\n\u001B[1;32m      9\u001B[0m sentiment_pipeline \u001B[38;5;241m=\u001B[39m pipeline(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msentiment-analysis\u001B[39m\u001B[38;5;124m'\u001B[39m, model\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfiniteautomata/bertweet-base-sentiment-analysis\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     11\u001B[0m \u001B[38;5;66;03m# Apply the pipeline to each review\u001B[39;00m\n\u001B[0;32m---> 12\u001B[0m sentiments \u001B[38;5;241m=\u001B[39m df\u001B[38;5;241m.\u001B[39mrdd\u001B[38;5;241m.\u001B[39mmap(\u001B[38;5;28;01mlambda\u001B[39;00m row: sentiment_pipeline(row[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mreview_body\u001B[39m\u001B[38;5;124m'\u001B[39m])[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlabel\u001B[39m\u001B[38;5;124m'\u001B[39m])\u001B[38;5;241m.\u001B[39mtoDF([\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msentiment\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[1;32m     14\u001B[0m \u001B[38;5;66;03m# Add the sentiment column back to the original dataframe\u001B[39;00m\n\u001B[1;32m     15\u001B[0m df \u001B[38;5;241m=\u001B[39m df\u001B[38;5;241m.\u001B[39mwithColumn(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msentiment\u001B[39m\u001B[38;5;124m'\u001B[39m, sentiments[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msentiment\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:103\u001B[0m, in \u001B[0;36m_monkey_patch_RDD.<locals>.toDF\u001B[0;34m(self, schema, sampleRatio)\u001B[0m\n\u001B[1;32m     75\u001B[0m \u001B[38;5;129m@no_type_check\u001B[39m\n\u001B[1;32m     76\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtoDF\u001B[39m(\u001B[38;5;28mself\u001B[39m, schema\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, sampleRatio\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m     77\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     78\u001B[0m \u001B[38;5;124;03m    Converts current :class:`RDD` into a :class:`DataFrame`\u001B[39;00m\n\u001B[1;32m     79\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    101\u001B[0m \u001B[38;5;124;03m    [Row(name='Alice', age=1)]\u001B[39;00m\n\u001B[1;32m    102\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 103\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43msparkSession\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcreateDataFrame\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mschema\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msampleRatio\u001B[49m\u001B[43m)\u001B[49m\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:972\u001B[0m, in \u001B[0;36mSparkSession.createDataFrame\u001B[0;34m(self, data, schema, samplingRatio, verifySchema)\u001B[0m\n\u001B[1;32m    967\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_pandas \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data, pd\u001B[38;5;241m.\u001B[39mDataFrame):\n\u001B[1;32m    968\u001B[0m     \u001B[38;5;66;03m# Create a DataFrame from pandas DataFrame.\u001B[39;00m\n\u001B[1;32m    969\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m(SparkSession, \u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39mcreateDataFrame(  \u001B[38;5;66;03m# type: ignore[call-overload]\u001B[39;00m\n\u001B[1;32m    970\u001B[0m         data, schema, samplingRatio, verifySchema\n\u001B[1;32m    971\u001B[0m     )\n\u001B[0;32m--> 972\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_create_dataframe\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    973\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mschema\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msamplingRatio\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverifySchema\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# type: ignore[arg-type]\u001B[39;49;00m\n\u001B[1;32m    974\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1020\u001B[0m, in \u001B[0;36mSparkSession._create_dataframe\u001B[0;34m(self, data, schema, samplingRatio, verifySchema)\u001B[0m\n\u001B[1;32m   1018\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1019\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data, RDD):\n\u001B[0;32m-> 1020\u001B[0m         rdd, struct \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_createFromRDD\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmap\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprepare\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mschema\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msamplingRatio\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1021\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1022\u001B[0m         rdd, struct \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_createFromLocal(\u001B[38;5;28mmap\u001B[39m(prepare, data), schema)\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:607\u001B[0m, in \u001B[0;36mSparkSession._createFromRDD\u001B[0;34m(self, rdd, schema, samplingRatio)\u001B[0m\n\u001B[1;32m    603\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    604\u001B[0m \u001B[38;5;124;03mCreate an RDD for DataFrame from an existing RDD, returns the RDD and schema.\u001B[39;00m\n\u001B[1;32m    605\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    606\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m schema \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(schema, (\u001B[38;5;28mlist\u001B[39m, \u001B[38;5;28mtuple\u001B[39m)):\n\u001B[0;32m--> 607\u001B[0m     struct \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_inferSchema\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrdd\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msamplingRatio\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnames\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mschema\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    608\u001B[0m     converter \u001B[38;5;241m=\u001B[39m _create_converter(struct)\n\u001B[1;32m    609\u001B[0m     tupled_rdd \u001B[38;5;241m=\u001B[39m rdd\u001B[38;5;241m.\u001B[39mmap(converter)\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:560\u001B[0m, in \u001B[0;36mSparkSession._inferSchema\u001B[0;34m(self, rdd, samplingRatio, names)\u001B[0m\n\u001B[1;32m    558\u001B[0m prefer_timestamp_ntz \u001B[38;5;241m=\u001B[39m is_timestamp_ntz_preferred()\n\u001B[1;32m    559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m samplingRatio \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 560\u001B[0m     schema \u001B[38;5;241m=\u001B[39m \u001B[43m_infer_schema\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    561\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfirst\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    562\u001B[0m \u001B[43m        \u001B[49m\u001B[43mnames\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnames\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    563\u001B[0m \u001B[43m        \u001B[49m\u001B[43minfer_dict_as_struct\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minfer_dict_as_struct\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    564\u001B[0m \u001B[43m        \u001B[49m\u001B[43mprefer_timestamp_ntz\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mprefer_timestamp_ntz\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    565\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    566\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m _has_nulltype(schema):\n\u001B[1;32m    567\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m row \u001B[38;5;129;01min\u001B[39;00m rdd\u001B[38;5;241m.\u001B[39mtake(\u001B[38;5;241m100\u001B[39m)[\u001B[38;5;241m1\u001B[39m:]:\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/types.py:1374\u001B[0m, in \u001B[0;36m_infer_schema\u001B[0;34m(row, names, infer_dict_as_struct, prefer_timestamp_ntz)\u001B[0m\n\u001B[1;32m   1371\u001B[0m     items \u001B[38;5;241m=\u001B[39m \u001B[38;5;28msorted\u001B[39m(row\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__dict__\u001B[39m\u001B[38;5;241m.\u001B[39mitems())\n\u001B[1;32m   1373\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1374\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCan not infer schema for type: \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m \u001B[38;5;28mtype\u001B[39m(row))\n\u001B[1;32m   1376\u001B[0m fields \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m   1377\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m items:\n\n\u001B[0;31mTypeError\u001B[0m: Can not infer schema for type: <class 'str'>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Part 3: Next Steps"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"15e0bb5c-4bfa-4da5-8a1e-df7510edf3e7","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["* Create new feature for the length of the review.\n* Create new feature for the word count of the review.\n* Single variable visualization with Plotly\n* The distribution of review ratings\n* The distribution of reviews lengths\n* The distribution of reviews word counts\n* The distribution of topics, e.g., Books on Machine Learing, Fiction, etc.\n* Top 20 one-words in review after removing stop words\n* Top 20 two-words (bigrams) used in reviews after removing stop words\n* Top 20 trigrams used in reviews after removing stop words\n* The distribution of top parts-of-speech (PoS) in the review corpus\n* Sentiment Polarity Boxlet of Books Topics - Ideally Machine Learning"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"dfcd35ee-f45e-4c94-9245-ddb35cae7e42","inputWidgets":{},"title":""}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Product-Transformer","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":1382591049369162}},"nbformat":4,"nbformat_minor":0}
